{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Подгрузка библиотек",
   "id": "aab0f35928d0f29c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "c5c8536db2490917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# df_path = '/content/drive/MyDrive/hacaton_1/SERGEY'"
   ],
   "id": "17c9580c10b03c7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_path = './data-copy'",
   "id": "3dbb017031b9f5a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tqdm.pandas(desc=\"Processing rows...\", ncols=100)",
   "id": "333bb80ff32b71ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Загрузка данных",
   "id": "3b1fb8bfc776152e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Исходный набор представляет собой открытые данные [Государственного каталога музейного фонда РФ Министерства культуры РФ](https://opendata.mkrf.ru/opendata/7705851331-museum-exhibits)",
   "id": "fa5cf76b24b349d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Подгрузка данных\n",
    "data = pd.read_csv(f'{df_path}/Russian_museum.csv', sep=';', encoding='utf-8')\n",
    "data.head()"
   ],
   "id": "5b4be1cdf6bb64af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data['Автор'].value_counts()",
   "id": "913f3775ce46831f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_surname(author: str) -> str:\n",
    "    if not isinstance(author, str) or not author.strip():\n",
    "        return \"Неизвестный\"\n",
    "    # Убираем кавычки и квадратные скобки\n",
    "    author = re.sub(r'^[\\[\\]\"]+|[\\[\\]\"]+$', '', author)\n",
    "    # Убираем даты (разные форматы записи)\n",
    "    author = re.sub(r'\\d{4}([–—,\\(\\)]*\\d{4})?', '', author)\n",
    "    # Убираем инициалы\n",
    "    author = re.sub(r'\\b[А-ЯЁ]\\.? ?[А-ЯЁ]\\.? ?', '', author)\n",
    "    # Убираем ненужные слова: \"г.\", \"род.\", \"ум.\", \"по проекту\", \"оглы\" и т. д.\n",
    "    author = re.sub(r'\\b(г\\.|род\\.|ум\\.|по|проекту|оглы|фон|де|мастер|скульптор|гравёр|художник|автор)\\b', '', author, flags=re.IGNORECASE)\n",
    "    # Убираем оставшиеся символы пунктуации\n",
    "    author = re.sub(r'[^А-Яа-яЁё-]', ' ', author)\n",
    "    # Убираем лишние пробелы\n",
    "    author = re.sub(r'\\s+', ' ', author).strip()\n",
    "    # Если после очистки ничего не осталось, возвращаем \"Неизвестный\"\n",
    "    if not author:\n",
    "        return \"Неизвестный\"\n",
    "    # Оставляем только первую часть имени (фамилию)\n",
    "    surname = author.split()[0]\n",
    "    return surname"
   ],
   "id": "a21554785a26044a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Применяем функцию\n",
    "data['Фамилия'] = data['Автор'].progress_apply(extract_surname)\n",
    "\n",
    "data['Фамилия'].value_counts()"
   ],
   "id": "8665e47156b8ef4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Очистка данных",
   "id": "a1675f3c827a80b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Удаление дубликатов\n",
    "data_cleaned = data.drop_duplicates()\n",
    "\n",
    "# Сброс индексов после удаления дубликатов\n",
    "data_cleaned = data_cleaned.reset_index(drop=True)"
   ],
   "id": "e9429b74d8014584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_cleaned",
   "id": "6356ff3e69b12c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_graph(top_5, column):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=top_5.index, y=top_5.values, palette='viridis')\n",
    "    plt.title(f'Top 5 Frequent Values in {column}')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ],
   "id": "a910cdb6cdc60157"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " for column in data_cleaned.columns:\n",
    "    print(column)\n",
    "    if data[column].dtype == 'object' or data[column].dtype.name == 'category':\n",
    "        # Получаем топ-5 частых значений для каждого категориального признака\n",
    "        top_5 = data_cleaned[column].value_counts().head(5)\n",
    "        get_graph(top_5, column)\n"
   ],
   "id": "a2da8ae9ebb2382e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_cleaned.info()",
   "id": "ffa332dcc85f9a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Исключаем неинформативные признаки\n",
    "cols_for_drop = ['Музей', 'Строковое описание размеров', 'Количество составляющих', 'Инвентарный номер', 'Номер по ГИК',\n",
    "                 'Ширина', 'Длина', 'Единица измерения размера', 'Место создания', 'Количество составляющих',\n",
    "                 'Ключевые слова', 'URL предмета на сайте музея', 'Высота', 'Идентификатор статуса предмета',\n",
    "                 'Краткое описание истории бытования (провенанс) предмета',\n",
    "                 'Точность задания времени (день, месяц, год, век, эпоха).',\n",
    "                 'Интервал времени создания предмета (начало)',\n",
    "                 'Интервал времени создания предмета (окончание)',\n",
    "\n",
    "                 ]\n",
    "data_cleaned.drop(cols_for_drop, axis=1, inplace=True)"
   ],
   "id": "575f68bce982c9c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_cleaned",
   "id": "f992a3cf7e6fdf58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_cleaned.columns",
   "id": "ba0fb3d30848b83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Переименовываем столбцы\n",
    "data_cleaned = data_cleaned.rename(columns={\n",
    "    'Наименование предмета': 'title',\n",
    "    'Автор': 'author_full_name',\n",
    "    'Фамилия': 'author',\n",
    "    'Items': 'items',\n",
    "    'Описание': 'description',\n",
    "    'Изображение': 'image_url',\n",
    "    'Регистрационный номер Госкаталога': 'catalog_num',\n",
    "    'Дата регистрации записи.': 'registration_date',\n",
    "    'Типология': 'typology',\n",
    "    'Дата создания предмета (строка)': 'creation_date',\n",
    "\n",
    "})"
   ],
   "id": "64b1508c840dad9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_cleaned.columns",
   "id": "b0c8a56726bacd34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Выбор графики и живописи",
   "id": "66e9ac6c80774d51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_cleaned['typology'].value_counts()",
   "id": "93d4198c90c5bf06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Приводим столбец 'Типология' к нижнему регистру и удаляем лишние пробелы\n",
    "data_cleaned['typology'] = data_cleaned['typology'].str.strip().str.lower()\n",
    "\n",
    "# Выбираем строки с типологией 'графика' или 'живопись'\n",
    "picture_data = data_cleaned[data_cleaned['typology'].isin(['графика', 'живопись'])].reset_index(drop=True)"
   ],
   "id": "e472942b143ee4ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "picture_data",
   "id": "14aafb488331c7a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Выбор данных, которые не относятся к 'графика' и 'живопись'\n",
    "other_data = data_cleaned[~data_cleaned['typology'].isin(['графика', 'живопись'])]\n",
    "other_data.to_csv(f'{df_path}/other_data.csv', index=False)"
   ],
   "id": "6916a0e7ba8e4377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Очистка столбца Материалы",
   "id": "8df5f8089010c5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "picture_data['items'] = picture_data['items'].str.replace('[\"', '').str.replace('\"]', '')",
   "id": "8d4b8865fcb3a92f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "picture_data['items']",
   "id": "3266a0976dd3331c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "synonyms = {\n",
    "    r'^графитный карандаш$': 'карандаш',\n",
    "    r'^цветной карандаш$': 'карандаш',\n",
    "    r'^карандаш графитовый$': 'карандаш',\n",
    "    r'^карандаш графитный$': 'карандаш',\n",
    "    r'^цифровая печать$': 'печать',\n",
    "    r'^типографская печать$': 'печать',\n",
    "    r'^альбуминовая печать$': 'печать',\n",
    "    r'^лит$': 'литография',\n",
    "    r'^литография с тоном$': 'литография',\n",
    "    r'^литография цветная$': 'литография',\n",
    "    r'^бумага верже$': 'бумага',\n",
    "    r'^роспись полихромная$': 'роспись',\n",
    "    r'^б.$': 'бумага',\n",
    "    r'^черные чернила$': 'тушь',\n",
    "    r'^акв$': 'акварель',\n",
    "    r'^акв\\.$': 'акварель',\n",
    "    r'^гравюра пунктиром$': 'гравюра',\n",
    "    r'^гравюра на дереве$': 'гравюра',\n",
    "    r'^карандаш итальянский$': 'карандаш',\n",
    "    r'^гравюра резцом$': 'гравюра',\n",
    "    r'^бумага на картоне$': 'бумага',\n",
    "    r'^бумага серая$': 'бумага',\n",
    "    r'^черный карандаш$': 'карандаш',\n",
    "    r'^граф\\. кар\\.$': 'карандаш',\n",
    "    r'^карандаши цветные$': 'карандаш',\n",
    "    r'^карандаш черный$': 'карандаш',\n",
    "    r'^акварель черная$': 'акварель',\n",
    "    r'^линогравюра$': 'гравюра',\n",
    "    r'^шариковая ручка$': 'чернила',\n",
    "    r'^хромолитография$': 'литография',\n",
    "    r'^автолитография$': 'литография',\n",
    "    r'^литография раскрашенная$': 'литография',\n",
    "    r'^фанера$': 'дерево',\n",
    "    r'^акварельарель черная$': 'акварель',\n",
    "    r'^цветная литография$': 'литография',\n",
    "    r'^граф\\. кар$': 'карандаш',\n",
    "    r'^холст на картоне$': 'холст',\n",
    "    r'^карандаш угольный$': 'карандаш',\n",
    "    r'^бумага цветная$': 'бумага',\n",
    "    r'^бумага желтая$': 'бумага',\n",
    "    r'^бумага черная$': 'бумага',\n",
    "    r'^черная акварель$': 'акварель',\n",
    "    r'^акварель атинта$': 'акварель',\n",
    "    r'^\\\\nмасло$': 'масло',\n",
    "    r'^гравюра цветная$': 'гравюра',\n",
    "    r'^итальянский карандаш$': 'карандаш',\n",
    "    r'^гравюра раскрашенная$': 'гравюра',\n",
    "    r'^монотипия цветная$': 'монотипия',\n",
    "    r'^уголь прессованный$': 'уголь',\n",
    "    r'^граф\\.кар\\.$': 'картон',\n",
    "    r'^бумага желтоватая$': 'бумага',\n",
    "    r'^бумага коричневая$': 'бумага',\n",
    "    r'^цветные карандаши$': 'карандаш',\n",
    "    r'^карандаш свинцовый$': 'карандаш',\n",
    "    r'^холст,масло$': 'масло',\n",
    "    r'^масляная пастель$': 'пастель',\n",
    "    r'^наклеенная на белую бумагу$': 'аппликация',\n",
    "    r'^наклейки$': 'аппликация',\n",
    "    r'^бумага мелованная$': 'бумага',\n",
    "    r'^ксилография цветная$': 'ксилография',\n",
    "    r'^карандаш литографский$': 'литография',\n",
    "    r'^тушь цветная$': 'тушь',\n",
    "    r'^мягкий лак$': 'лак',\n",
    "    r'^гравюра на картоне$': 'гравюра',\n",
    "    r'^картон мелованный$': 'картон',\n",
    "    r'^бронзовая краска$': 'бронза',\n",
    "    r'^к\\.$': 'картон',\n",
    "    r'^бумага голубая$': 'бумага',\n",
    "    r'^бумага оберточная$': 'бумага',\n",
    "    r'^слоновая кость\\\\nминиатюра$': 'миниатюра',\n",
    "    r'^автолитография цветная$': 'литография',\n",
    "    r'^гравюра резцом раскрашенная$': 'гравюра',\n",
    "    r'^линогравюра цветная$': 'гравюра',\n",
    "    r'^черн. акв$': 'акварель',\n",
    "    r'^холст,\\\\nмасло$': 'масло',\n",
    "    r'^цв. кар$': 'картон',\n",
    "    r'^бумага зеленая$': 'бумага'\n",
    "\n",
    "}\n"
   ],
   "id": "d7e3b2321ac58fda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_material(material, synonyms):\n",
    "    if pd.isna(material):\n",
    "        material = ''\n",
    "    else:\n",
    "        material = material.strip().lower()\n",
    "    for old, new in synonyms.items():\n",
    "        material = re.sub(old, new, material)\n",
    "    return material"
   ],
   "id": "42b8324423c2c96e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_top_materials(dataframe, column_name, synonyms, top_n=50):\n",
    "    all_materials = dataframe[column_name].str.split(', ').explode()\n",
    "    all_materials_cleaned = all_materials.progress_apply(lambda material: clean_material(material, synonyms))\n",
    "    material_counts = Counter(all_materials_cleaned)\n",
    "    counter = pd.DataFrame.from_dict(material_counts, orient='index', columns=['Count'])\n",
    "    counter_cleaned = counter.sort_values(by='Count', ascending=False)\n",
    "    top_materials = counter_cleaned.head(top_n)\n",
    "\n",
    "    return top_materials.index.tolist()\n"
   ],
   "id": "cf924617ab1ca8bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_50 = analyze_top_materials(picture_data, 'items', synonyms)\n",
    "top_50"
   ],
   "id": "34a8e5a862344f2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Функция для обработки строки\n",
    "def process_items(item):\n",
    "    if pd.isna(item):\n",
    "        return ''\n",
    "    # Разделяем строку на отдельные материалы\n",
    "    materials = item.split(', ')\n",
    "    # Заменяем синонимы\n",
    "    materials = [material for material in materials if material in top_50]\n",
    "    # Удаляем дубликаты и сортируем\n",
    "    return ','.join(sorted(set(materials)))"
   ],
   "id": "18ccd8c35172a70b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Преобразуем столбец items в key_words\n",
    "picture_data['key_materials'] = picture_data['items'].progress_apply(process_items)"
   ],
   "id": "b62ce2581f1b4be1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "picture_data['key_materials']",
   "id": "d1bb9cb188c1502a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "picture_data.to_csv(f'{df_path}/picture_data_with_key_material.csv', index=False, encoding='utf-8', sep=';')",
   "id": "8b1274b12fb8f6c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Очистка столбца с датами",
   "id": "3f5a18961c7ad089"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# функция для предобработки дат\n",
    "def process_date(date_str):\n",
    "    try:\n",
    "        # Проверка на NaN\n",
    "        if pd.isna(date_str):\n",
    "            return \"Неизвестно\"\n",
    "\n",
    "        # Преобразуем строку\n",
    "        date_str = re.sub(r\"[^\\w\\s\\-]\", \"\", date_str).lower()\n",
    "        date_str = date_str.replace(\"х\", \"х годов\")\n",
    "\n",
    "        # Обработка \"конец\" (например, \"конец 1920-х\" или \"конец 1900-х\")\n",
    "        if \"конец\" in date_str:\n",
    "            decades = re.findall(r\"\\d{3,4}\", date_str)\n",
    "            if len(decades) == 1:\n",
    "                start = int(decades[0])\n",
    "                if start % 100 != 0:\n",
    "                    return f\"{start + 6}-{start + 9}\"  # для конца десятилетия, например, конец 1920-х -> 1926-1929\n",
    "                else:\n",
    "                    return f\"{start + 96}-{start + 99}\"  # для конца десятилетия после 1900-х, например, конец 1900-х -> 1996-1999\n",
    "            elif len(decades) == 2:\n",
    "                return f\"{decades[0]}-{decades[1]}\"\n",
    "\n",
    "        # Обработка \"начало\" (например, \"начало 1920-х\")\n",
    "        if \"начало\" in date_str:\n",
    "            decades = re.findall(r\"\\d{3,4}\", date_str)\n",
    "            if len(decades) == 1:\n",
    "                start = int(decades[0])\n",
    "                return f\"{start}-{start + 9}\"  # для начала десятилетия, например, начало 1920-х -> 1920-1929\n",
    "            elif len(decades) == 2:\n",
    "                return f\"{decades[0]}-{decades[1]}\"\n",
    "\n",
    "        # Обработка \"середина\" (например, \"середина 1920-х\")\n",
    "        if \"середина\" in date_str:\n",
    "            decades = re.findall(r\"\\d{3,4}\", date_str)\n",
    "            if len(decades) == 1:\n",
    "                start = int(decades[0])\n",
    "                return f\"{start + 4}-{start + 5}\"\n",
    "\n",
    "        # Обработка \"1-я половина\" и \"2-я половина\"\n",
    "        if \"1-я половина\" in date_str or \"1-ая половина\" in date_str:\n",
    "            decades = re.findall(r\"\\d{3,4}\", date_str)\n",
    "            if len(decades) == 1:\n",
    "                start = int(decades[0])\n",
    "                return f\"{start}-{start + 49}\"\n",
    "\n",
    "        if \"2-я половина\" in date_str or \"2-ая половина\" in date_str:\n",
    "            decades = re.findall(r\"\\d{3,4}\", date_str)\n",
    "            if len(decades) == 1:\n",
    "                start = int(decades[0])\n",
    "                return f\"{start + 50}-{start + 99}\"\n",
    "\n",
    "        # Обработка диапазонов\n",
    "        if \"-\" in date_str:\n",
    "            years = re.findall(r\"\\d{4}\", date_str)\n",
    "            if len(years) == 2:\n",
    "                start_year = int(years[0])\n",
    "                end_year = int(years[1])\n",
    "                if start_year >= 1000 and end_year <= 3000:\n",
    "                    return f\"{start_year}-{end_year}\"\n",
    "            elif len(years) == 1:\n",
    "                return f\"{years[0]}\"\n",
    "\n",
    "        # Обработка \"около\"\n",
    "        if \"ок\" in date_str or \"примерно\" in date_str:\n",
    "            years = re.findall(r\"\\d{4}\", date_str)\n",
    "            if len(years) == 1:\n",
    "                return years[0]\n",
    "\n",
    "        # Обработка одиночных годов\n",
    "        years = re.findall(r\"\\d{4}\", date_str)\n",
    "        if len(years) == 1:\n",
    "            return years[0]\n",
    "        # Если ничего не найдено\n",
    "        return \"Неизвестно\"\n",
    "    except:\n",
    "        return \"Ошибка\"\n"
   ],
   "id": "a92a6d31351e85ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Обработка всех строк\n",
    "picture_data['date_cleaned'] = picture_data['creation_date'].progress_apply(process_date)\n",
    "picture_data[['date_cleaned', 'creation_date']]"
   ],
   "id": "32cf529d93ca5308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def categorize_date(date_str):\n",
    "    try:\n",
    "        # Проверка на NaN и ошибки\n",
    "        if pd.isna(date_str) or date_str == \"Неизвестно\":\n",
    "            return \"Неизвестно\"\n",
    "\n",
    "        # Обработка диапазонов (например, 1920-1930)\n",
    "        if \"-\" in date_str:\n",
    "            years = re.findall(r\"\\d{4}\", date_str)\n",
    "            if len(years) == 2:\n",
    "                start_year = int(years[0])\n",
    "                end_year = int(years[1])\n",
    "                avg_year = (start_year + end_year) // 2\n",
    "            else:\n",
    "                return \"Неизвестно\"\n",
    "        else:\n",
    "            avg_year = int(date_str)\n",
    "\n",
    "        # Определяем век на основе среднего года\n",
    "        century = (avg_year // 100) + 1\n",
    "\n",
    "        # Определяем, первая или вторая половина века\n",
    "        if avg_year % 100 <= 50:\n",
    "            return f\"1-я пол.{century}\"\n",
    "        else:\n",
    "            return f\"2-я пол.{century}\"\n",
    "\n",
    "    except:\n",
    "        return f\"Ошибка\""
   ],
   "id": "fa9ffbc21803ac92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "picture_data['date_category'] = picture_data['date_cleaned'].progress_apply(categorize_date)\n",
    "picture_data[['date_category', 'date_cleaned', 'creation_date']]"
   ],
   "id": "2fb4c8e59124877d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "picture_data.to_csv(f'{df_path}/picture_data_with_date_and_materials.csv', sep=';', encoding='utf-8', index=False)",
   "id": "8420777faaa0a2a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Поиск ключевых слов по названию и описанию",
   "id": "75bce9f3dad2b7fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.lang.ru.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "id": "736d73732e26b95c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python -m spacy download ru_core_news_sm",
   "id": "d0d130ad9ba4f2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Загрузка модели spacy для русского языка\n",
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ],
   "id": "a3c3e5e0796bc216"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Функция для лемматизации и очистки текста\n",
    "def lemmatize_text_spacy(text):\n",
    "    doc = nlp(text.lower())  # Приводим к нижнему регистру\n",
    "    lemmatized_words = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha  # Убираем числа и знаки препинания\n",
    "        and len(token.lemma_) > 1  # Исключаем отдельные буквы\n",
    "        and token.lemma_ not in STOP_WORDS  # Исключаем стоп-слова\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)"
   ],
   "id": "5983e4e9a1d7f495"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_keywords_with_tfidf(df, top_n=15):\n",
    "    # Объединяем описание и название в единый текст и лемматизируем\n",
    "    df['combined_text'] = df['description'] + ' ' + df['title']\n",
    "    df['lemmatized_text'] = df['combined_text'].apply(lemmatize_text_spacy)\n",
    "\n",
    "    # Инициализируем TF-IDF векторизатор с русскими стоп-словами\n",
    "    vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['lemmatized_text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    df['key_words'] = ''\n",
    "\n",
    "    for index in tqdm(range(len(df)), total=len(df), desc=\"Processing rows\", ncols=100):\n",
    "        # Получаем TF-IDF веса для текущей строки\n",
    "        tfidf_vector = tfidf_matrix[index].toarray().flatten()\n",
    "        top_indices = tfidf_vector.argsort()[-top_n:][::-1]\n",
    "\n",
    "        # Извлекаем ключевые слова\n",
    "        key_words = [feature_names[i] for i in top_indices if tfidf_vector[i] > 0]\n",
    "\n",
    "        # Сохраняем ключевые слова\n",
    "        df.at[index, 'key_words'] = ','.join(key_words)\n",
    "\n",
    "    # Удаляем временные колонки\n",
    "    df.drop(columns=['combined_text', 'lemmatized_text'], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "a20dfd309db7a579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = extract_keywords_with_tfidf(picture_data)",
   "id": "30f3b877315392a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Сохранение результата в CSV файл\n",
    "df.to_csv(f'{df_path}/picture_data_with_keywords.csv', index=False, sep=';', encoding='utf-8')"
   ],
   "id": "5c961701557c5f3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[['key_words', 'title', 'description']]",
   "id": "2e343c85b8cdf22e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Сохранение датафрейма для кластеризации и загрузка в бд",
   "id": "8e1db27a5aeb8232"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.drop(['date_cleaned', 'key_materials', 'creation_date'], inplace=True, axis=1)",
   "id": "555591448de988ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "columns_for_db = [\n",
    "    'image_url', 'catalog_num', 'registration_date',\n",
    "]"
   ],
   "id": "7477f6b2dae50381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def reformate_url(column):\n",
    "    if pd.isna(column):\n",
    "        return None\n",
    "    match = re.search(r'\"url\":\"(http[s]?://[^\"]+)\"', column)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ],
   "id": "5a4f130f5215da86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['image_url'] = df['image_url'].apply(reformate_url)",
   "id": "11511284933908cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['image_url'].head()",
   "id": "8fae342ed69c7fb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_csv(f'{df_path}/data_for_database.csv', index=False, sep=';', encoding='utf-8')",
   "id": "c0f016b6d7b47779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_csv(f'{df_path}/data_for_database.csv', sep=';', encoding='utf-8')",
   "id": "13455227433b9120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.drop(columns_for_db, inplace=True, axis=1)",
   "id": "b5f41fae08ef416d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.columns",
   "id": "530dba49fe20f254"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.rename(columns={\n",
    "    'author_lastname': 'author',\n",
    "}, inplace=True)"
   ],
   "id": "f07ae8ecfa2710cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_csv(f'{df_path}/data_for_clasterisation.csv', index=False, sep=';', encoding='utf-8')",
   "id": "930129f83c4cb39e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df",
   "id": "85e85c54a9bb3850"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Рекомендательная система",
   "id": "62fb1c545dbdf2d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n"
   ],
   "id": "9494e6070e6b95c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "folder_path = './data-copy'\n",
    "df_path = folder_path + '/data_for_clasterisation.csv'"
   ],
   "id": "52bb17fa8e0c8235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Инициализация tqdm_pandas\n",
    "tqdm.pandas()"
   ],
   "id": "bf45e4c78dbc7150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(df_path, sep=';', encoding='utf-8')\n",
    "df"
   ],
   "id": "766588d1b2f7e415"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# очистка пропусков\n",
    "df['key_words'] = df['key_words'].fillna('')\n",
    "df['author'] = df['author'].fillna('')\n",
    "df['key_words'] = df['key_words'] + ',' + df['author']"
   ],
   "id": "8f640642f27ed1bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Кодирование признаков",
   "id": "f8aa6832f10d6be9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Загрузка предобученной модели для русского языка\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ],
   "id": "b4842bf09c99d763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Функция для получения эмбеддингов с помощью RuBERT\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings"
   ],
   "id": "5112e76079befc35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_item_embeddings(data):\n",
    "    descriptions = data['key_words'].tolist()\n",
    "    embeddings = []\n",
    "    # Получаем эмбеддинги для каждого экспоната\n",
    "    for desc in tqdm(descriptions, desc=\"Генерация эмбеддингов для описаний\", unit=\"экспонат\"):\n",
    "        embedding = get_embeddings([desc])\n",
    "        embeddings.append(embedding)\n",
    "    # Преобразуем список эмбеддингов в numpy массив\n",
    "    embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "    return embeddings"
   ],
   "id": "a3ccdced768a47cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Функция для дополнения эмбеддинга запроса до требуемой размерности\n",
    "def resize_embedding(query_embedding, target_dim):\n",
    "    current_dim = query_embedding.shape[1]\n",
    "\n",
    "    # Если текущая размерность уже совпадает с целевой, возвращаем эмбеддинг без изменений\n",
    "    if current_dim == target_dim:\n",
    "        return query_embedding\n",
    "\n",
    "    # Если текущая размерность меньше целевой, дополняем нулями\n",
    "    if current_dim < target_dim:\n",
    "        padding = np.zeros((query_embedding.shape[0], target_dim - current_dim))\n",
    "        return np.hstack([query_embedding, padding])\n",
    "\n",
    "    # Если текущая размерность больше целевой, обрезаем лишние элементы\n",
    "    return query_embedding[:, :target_dim]\n"
   ],
   "id": "284027606c28dc43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Функция визуализации кластеров\n",
    "def visualize_clusters(features, data, method=\"PCA\"):\n",
    "    # Снижение размерности\n",
    "    if method == \"PCA\":\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == \"TSNE\":\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    else:\n",
    "        raise ValueError(\"Метод должен быть 'PCA' или 'TSNE'\")\n",
    "\n",
    "    reduced_features = reducer.fit_transform(features)\n",
    "\n",
    "    # Создание DataFrame для визуализации\n",
    "    visualization_df = pd.DataFrame(reduced_features, columns=[\"Dim1\", \"Dim2\"])\n",
    "    visualization_df['cluster'] = data['cluster']\n",
    "\n",
    "    # Визуализация кластеров\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        x=\"Dim1\", y=\"Dim2\",\n",
    "        hue=\"cluster\",\n",
    "        palette=\"tab10\",\n",
    "        data=visualization_df,\n",
    "        legend=\"full\"\n",
    "    )\n",
    "    plt.title(f\"Визуализация кластеров ({method})\", fontsize=16)\n",
    "    plt.xlabel(\"Первая компонента\", fontsize=12)\n",
    "    plt.ylabel(\"Вторая компонента\", fontsize=12)\n",
    "    plt.legend(title=\"Кластеры\")\n",
    "    plt.show()\n"
   ],
   "id": "aa327917aad89a29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def plot_silhouette_score(X, min_clusters=2, max_clusters=10):\n",
    "    scores = []\n",
    "    cluster_range = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "    for k in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Построение графика\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cluster_range, scores, marker='o', linestyle='--')\n",
    "    plt.xlabel(\"Число кластеров\")\n",
    "    plt.ylabel(\"Силуэтный коэффициент\")\n",
    "    plt.title(\"Определение оптимального числа кластеров\")\n",
    "    plt.xticks(cluster_range)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "1289a00cd37151e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "embeddings = get_item_embeddings(df)",
   "id": "75a75e8ac9564ba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "embeddings",
   "id": "d9c3655d859a2879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.save(folder_path + '/embeddings.npy', embeddings)",
   "id": "8be7d1f40a0d0ebc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Сохранение в CSV\n",
    "with open(folder_path + 'embeddings.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(embeddings)"
   ],
   "id": "9bd393ccfa41acb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "embeddings.shape",
   "id": "d673d0655d056e92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install pyarrow",
   "id": "8b141d59aeacbd14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['embeddings'] = list(embeddings)\n",
    "# Сохраняем в Parquet (меньше места, быстрее загрузка)\n",
    "df.to_parquet(folder_path + \"/dataset_with_embeddings.parquet\", index=False, engine=\"pyarrow\")"
   ],
   "id": "882cb32bf1e18b81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_parquet(folder_path + \"/dataset_with_embeddings.parquet\")\n",
    "df"
   ],
   "id": "75325d99d0996f31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Рекомендатлеьная система",
   "id": "b4ad9a7e571c817a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embeddings = df['embeddings']\n",
    "embeddings.dtype"
   ],
   "id": "d138ad0ccdb5a1f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_kmeans_model(embeddings, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "    return kmeans"
   ],
   "id": "a6acbdce3057d235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_silhouette_score(embeddings)",
   "id": "c517f30cf2308382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kmeans_model = create_kmeans_model(embeddings, n_clusters=5)\n",
    "kmeans_model.cluster_centers_.shape"
   ],
   "id": "4d54d6674c10ecc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Сохранение модели\n",
    "dump(kmeans_model, './data-copy/kmeans_model2.joblib')"
   ],
   "id": "5517184733a0558d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Загрузка модели\n",
    "kmeans_model = load('./data-copy/kmeans_model2.joblib')\n",
    "kmeans_model.cluster_centers_.shape"
   ],
   "id": "94747428a14c07ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Прогнозируем, к какому кластеру принадлежит каждый экспонат\n",
    "cluster_labels = kmeans_model.predict(embeddings)\n",
    "\n",
    "# Добавляем информацию о кластере в данные\n",
    "df['cluster'] = cluster_labels"
   ],
   "id": "50dab7e268ebb23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Визуализация кластеров с использованием PCA\n",
    "visualize_clusters(embeddings, df, method=\"PCA\")"
   ],
   "id": "ce4ce4ffbef5ac1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Визуализация кластеров с использованием t-SNE\n",
    "visualize_clusters(embeddings, df, method=\"TSNE\")"
   ],
   "id": "3261e39f44bafde7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_parquet(folder_path + 'data_with_clasters2.parquet', index=False, engine=\"pyarrow\")",
   "id": "a85e5ecedb9b71b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_parquet(folder_path + 'data_with_clasters2.parquet')\n",
    "df"
   ],
   "id": "138472b6f30643f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_to_merge = pd.read_csv('./data/data_for_database.csv', sep=';', encoding='utf-8')",
   "id": "3b6765daecfe2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_to_merge",
   "id": "79e8c6986d697bb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['author'].rename('author_lastname')\n",
    "df['author_name'] = df_to_merge['author_name']\n",
    "df['author_patronymic'] = df_to_merge['author_patronymic']\n",
    "df['catalog_num'] = df_to_merge['catalog_num']\n",
    "df['registration_date'] = df_to_merge['registration_date']\n",
    "df['image_url'] = df_to_merge['image_url']\n",
    "df[\"registration_date\"] = pd.to_datetime(df[\"registration_date\"])\n",
    "df"
   ],
   "id": "176622ef218a01de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_parquet(folder_path + '/data_for_database_final2.parquet', index=False, engine=\"pyarrow\")",
   "id": "a357aa0110ffee45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_parquet(folder_path + '/data_for_database_final2.parquet')",
   "id": "c01d8522911a03d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Сохранение в бд",
   "id": "572f5d6f0fb6be4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install python-dotenv sqlalchemy psycopg2",
   "id": "4282f39ee71009e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Загружаем переменные окружения из .env файла\n",
    "load_dotenv()\n",
    "\n",
    "# Получаем параметры из окружения\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USERNAME')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "\n",
    "# Формируем строку подключения для PostgreSQL\n",
    "DATABASE_URL = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Создание подключения к базе данных\n",
    "engine = create_engine(DATABASE_URL)"
   ],
   "id": "bcc0191cf45133e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ],
   "id": "f8eb6d06188aa031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['embeddings'] = df['embeddings'].apply(lambda x: x.tolist())",
   "id": "5f90df74530526cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Пример использования tqdm с to_sql\n",
    "try:\n",
    "    chunksize = 10  # Размер батча\n",
    "    num_chunks = len(df) // chunksize + 1\n",
    "\n",
    "    # Создаем прогресс-бар с tqdm\n",
    "    with tqdm(total=num_chunks, desc=\"Загрузка в PostgreSQL\") as pbar:\n",
    "        for start in range(0, len(df), chunksize):\n",
    "            df.iloc[start:start + chunksize].to_sql(\"images\", engine, if_exists=\"append\", index=False,\n",
    "                                                    chunksize=chunksize)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"✅ Датасет загружен в PostgreSQL\")\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"❌ Ошибка: {e}\")\n",
    "finally:\n",
    "    session.close()"
   ],
   "id": "f8807b69d90e6c8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Рекомендательная система KMeans",
   "id": "967698dcf71f4825"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Рекомендация экспонатов по кластеру\n",
    "def recommend_by_cluster(query, data, kmeans_model, n_recommendations=10):\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = get_embeddings([query]).numpy()\n",
    "\n",
    "    # Приводим эмбеддинг к типу float32\n",
    "    query_embedding = query_embedding.astype(np.float32)\n",
    "\n",
    "    # Определяем кластер для запроса\n",
    "    query_cluster = kmeans_model.predict(query_embedding)\n",
    "\n",
    "    # Рекомендуем экспонаты из того же кластера\n",
    "    recommended_items = data[data['cluster'] == query_cluster[0]].head(n_recommendations)\n",
    "    return recommended_items\n"
   ],
   "id": "1b7dc55b989998dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Пример: Рекомендуем экспонаты для пользователя с запросом\n",
    "user_query = \"картина, изображающая пейзаж с озером\"\n",
    "recommended_items = recommend_by_cluster(user_query, df, kmeans_model)\n",
    "recommended_items[['title', 'author', 'date_category']]"
   ],
   "id": "48f6e02832a60553"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Рекомендательная система на основе KMeans + KNN",
   "id": "85ddad4ef05da31b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def recommend_by_kmeans_knn(query, data, embeddings, kmeans_model, n_recommendations=10):\n",
    "    # Получаем целевую размерность из KMeans\n",
    "    target_dim = kmeans_model.cluster_centers_.shape[1]\n",
    "\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = get_embeddings([query]).numpy()\n",
    "\n",
    "    # Приводим эмбеддинг к нужной размерности\n",
    "    query_embedding_resized = resize_embedding(query_embedding, target_dim)\n",
    "\n",
    "    # Определяем кластер для запроса\n",
    "    query_cluster = kmeans_model.predict(query_embedding_resized)\n",
    "\n",
    "    # Получаем индексы объектов в этом кластере\n",
    "    cluster_indices = np.where(kmeans_model.labels_ == query_cluster)[0]\n",
    "\n",
    "    # Если embeddings - разреженная матрица, конвертируем в плотный массив\n",
    "    if isinstance(embeddings, np.ndarray):\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "    else:\n",
    "        cluster_embeddings = np.asarray(embeddings.todense()[cluster_indices])\n",
    "\n",
    "    # Создаем модель NearestNeighbors\n",
    "    nn_model = NearestNeighbors(n_neighbors=n_recommendations, metric='cosine')\n",
    "    nn_model.fit(cluster_embeddings)\n",
    "\n",
    "    # Находим ближайших соседей для запроса в этом кластере\n",
    "    distances, indices = nn_model.kneighbors(query_embedding_resized)\n",
    "\n",
    "    # Возвращаем рекомендованные объекты\n",
    "    recommended_items = data.iloc[cluster_indices[indices.flatten()]]\n",
    "\n",
    "    return recommended_items\n"
   ],
   "id": "4409250a89e56445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Пример: Рекомендуем экспонаты для пользователя с запросом\n",
    "user_query = \"картина, изображающая пейзаж с озером\"\n",
    "\n",
    "# Рекомендуем экспонаты, используя модель KMeans и KNN\n",
    "recommended_items = recommend_by_kmeans_knn(user_query, df, embeddings, kmeans_model)\n",
    "\n",
    "# Выводим рекомендованные экспонаты с нужными колонками\n",
    "recommended_items[['title', 'author', 'date_category']]"
   ],
   "id": "3f6d5f969b76beea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Функции для чат-бота",
   "id": "f911d3396cd5cf64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import joblib  # Для загрузки k-means модели\n",
    "\n",
    "# === Загрузка модели KMeans ===\n",
    "kmeans_model = joblib.load(folder_path + \"/kmeans_model.joblib\")\n",
    "\n",
    "# === Загрузка переменных окружения ===\n",
    "load_dotenv()\n",
    "\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USERNAME')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "\n",
    "DATABASE_URL = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# === Загрузка модели RuBERT ===\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model.eval()  # Переключаем в режим инференса\n",
    "\n",
    "# === Загрузка эмбеддингов из parquet ===\n",
    "df = pd.read_parquet(folder_path + '/data_for_database_final.parquet')\n",
    "df[\"id\"] = df.index\n",
    "\n",
    "# Преобразуем эмбеддинги в numpy-массив\n",
    "embeddings = np.vstack(df[\"embeddings\"].apply(np.array))  # Если хранятся как списки\n",
    "\n",
    "\n",
    "# === Функция для получения эмбеддингов запроса ===\n",
    "def get_embeddings(query):\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Усредняем по нужной размерности\n",
    "    return embeddings.cpu().numpy()  # Переводим в numpy\n",
    "\n",
    "\n",
    "# === Функция рекомендаций через KMeans + KNN ===\n",
    "def recommend_by_kmeans_knn(query, embeddings, kmeans_model, n_recommendations=5):\n",
    "    query_embedding = get_embeddings(query)  # Уже numpy\n",
    "    query_cluster = kmeans_model.predict(query_embedding)[0]  # Получаем кластер\n",
    "    cluster_indices = np.where(kmeans_model.labels_ == query_cluster)[0]  # Выбираем объекты из кластера\n",
    "\n",
    "    cluster_embeddings = embeddings[cluster_indices]  # Берем их эмбеддинги\n",
    "\n",
    "    # Создаем KNN-модель\n",
    "    nn_model = NearestNeighbors(n_neighbors=n_recommendations, metric='cosine')\n",
    "    nn_model.fit(cluster_embeddings)\n",
    "\n",
    "    distances, indices = nn_model.kneighbors(query_embedding)\n",
    "\n",
    "    recommended_indices = cluster_indices[indices.flatten()]  # Переводим в индексы исходного массива\n",
    "    return recommended_indices\n",
    "\n",
    "\n",
    "# === Функция получения экспонатов по индексам ===\n",
    "def get_exhibits_by_indices(indices):\n",
    "    return df.iloc[indices][[\"id\", \"title\", \"author\", \"date_category\"]].to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# === Пример использования ===\n",
    "user_query = \"картина, изображающая пейзаж с озером\"\n",
    "recommended_indices = recommend_by_kmeans_knn(user_query, embeddings, kmeans_model)\n",
    "\n",
    "# Получаем рекомендованные экспонаты\n",
    "recommended_exhibits = get_exhibits_by_indices(recommended_indices)\n",
    "print(recommended_exhibits, sep='\\n')\n"
   ],
   "id": "234ca47dd2897f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "import joblib  # Для загрузки k-means модели\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# === Загрузка модели KMeans ===\n",
    "kmeans_model = joblib.load(\"./data-copy/kmeans_model.joblib\")\n",
    "\n",
    "# === Загрузка переменных окружения ===\n",
    "load_dotenv()\n",
    "\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USERNAME')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "\n",
    "DATABASE_URL = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Создаем движок для подключения к базе данных\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# === Загрузка модели RuBERT ===\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# === Загрузка эмбеддингов из базы данных ===\n",
    "query = \"\"\"SELECT  catalog_num, title, author, date_category, embeddings FROM images_1\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Проверка типов данных\n",
    "print(df.dtypes)\n",
    "\n",
    "# Преобразуем эмбеддинги в numpy-массив\n",
    "# embeddings = np.vstack(df[\"embeddings\"].apply(np.array))  # Предполагаем, что эмбеддинги хранятся как списки\n",
    "# Функция для преобразования строк в массивы\n",
    "def convert_to_array(embedding_string):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(embedding_string))\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка преобразования: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# Применяем преобразование\n",
    "df['embeddings'] = df['embeddings'].apply(convert_to_array)\n",
    "\n",
    "# Удаляем пустые массивы, если это необходимо\n",
    "df = df[df['embeddings'].apply(lambda x: x.size > 0)]\n",
    "\n",
    "# Пробуем встраивание массивов в один, если они всех одного размера\n",
    "try:\n",
    "    embeddings = np.vstack(df['embeddings'].to_numpy())\n",
    "except ValueError as e:\n",
    "    print(f\"Ошибка при вертикальном сложении: {e}\")\n",
    "\n",
    "# === Функция для получения эмбеддингов запроса ===\n",
    "def get_embeddings(query):\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Усредняем по нужной размерности\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "# === Функция рекомендаций через KMeans + KNN ===\n",
    "def recommend_by_kmeans_knn(query, embeddings, kmeans_model, n_recommendations=10):\n",
    "    query_embedding = get_embeddings(query)  # Уже numpy\n",
    "    query_cluster = kmeans_model.predict(query_embedding)[0]  # Получаем кластер\n",
    "    cluster_indices = np.where(kmeans_model.labels_ == query_cluster)[0]  # Выбираем объекты из кластера\n",
    "\n",
    "    cluster_embeddings = embeddings[cluster_indices]  # Берем их эмбеддинги\n",
    "\n",
    "    # Создаем KNN-модель\n",
    "    nn_model = NearestNeighbors(n_neighbors=n_recommendations, metric='cosine')\n",
    "    nn_model.fit(cluster_embeddings)\n",
    "\n",
    "    distances, indices = nn_model.kneighbors(query_embedding)\n",
    "\n",
    "    recommended_indices = cluster_indices[indices.flatten()]\n",
    "    return recommended_indices\n",
    "\n",
    "\n",
    "# === Функция получения экспонатов по индексам ===\n",
    "def get_exhibits_by_indices(indices):\n",
    "    return df.iloc[indices][[\"catalog_num\", \"title\", \"author\", \"date_category\"]].to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# === Пример использования ===\n",
    "user_query = \"Хочу посмотреть изображение мужчины в историческом костюме\"\n",
    "recommended_indices = recommend_by_kmeans_knn(user_query, embeddings, kmeans_model)\n",
    "\n",
    "# Получаем рекомендованные экспонаты\n",
    "recommended_exhibits = get_exhibits_by_indices(recommended_indices)\n",
    "print(recommended_exhibits, sep='\\n')"
   ],
   "id": "b7a966581f0899ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install pgvector",
   "id": "1e8fe63a2dcace36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# последняя версия с БД",
   "id": "b95ee84b104878cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy.orm import sessionmaker"
   ],
   "id": "7ac83cd0b36bdd14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка переменных окружения ===\n",
    "load_dotenv()\n",
    "\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USERNAME')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "\n",
    "DATABASE_URL = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Создание подключения к БД\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ],
   "id": "10d92643d1693b45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка модели KMeans ===\n",
    "kmeans_model = joblib.load(\"./data-copy/kmeans_model2.joblib\")\n",
    "\n",
    "# === Загрузка модели RuBERT ===\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ],
   "id": "58142c04a80400d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция для получения эмбеддингов запроса ===\n",
    "def get_embeddings(query):\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()"
   ],
   "id": "f136066558ef4102"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка эмбеддингов из PostgreSQL ===\n",
    "def load_embeddings():\n",
    "    query = \"SELECT catalog_num, title, author, date_category, embeddings FROM images\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    df['embeddings'] = df['embeddings'].apply(lambda x: np.array(x))\n",
    "    return df\n",
    "\n",
    "df = load_embeddings()\n",
    "embeddings = np.vstack(df[\"embeddings\"].values)"
   ],
   "id": "3e6fee5670a5ffda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция рекомендаций через KMeans + KNN без использования эмбеддингов ===\n",
    "def recommend_by_kmeans_knn(query, kmeans_model, n_recommendations=5):\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = get_embeddings(query)\n",
    "    query_embedding = query_embedding.astype(np.float32)\n",
    "    # Определяем кластер для запроса\n",
    "    query_cluster = kmeans_model.predict(query_embedding)[0]\n",
    "\n",
    "    # Получаем индексы объектов в этом кластере\n",
    "    query_cluster_indices = np.where(kmeans_model.labels_ == query_cluster)[0]\n",
    "\n",
    "    # Строим модель KNN для ближайших соседей по кластеру (не загружая все эмбеддинги в память)\n",
    "    nn_model = NearestNeighbors(n_neighbors=n_recommendations, metric='cosine')\n",
    "\n",
    "    # Вместо загрузки всех эмбеддингов, подгружаем только те, что принадлежат кластеру\n",
    "    embeddings_query_cluster = get_embeddings_for_indices(query_cluster_indices)\n",
    "\n",
    "    nn_model.fit(embeddings_query_cluster)\n",
    "\n",
    "    # Находим ближайших соседей\n",
    "    distances, indices = nn_model.kneighbors(query_embedding)\n",
    "\n",
    "    # Получаем индексы ближайших объектов в кластере\n",
    "    recommended_indices = query_cluster_indices[indices.flatten()]\n",
    "\n",
    "    return recommended_indices"
   ],
   "id": "764cd4af334ad6ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Функция для получения эмбеддингов для выбранных индексов\n",
    "def get_embeddings_for_indices(indices):\n",
    "    # Получаем эмбеддинги из БД для объектов с данными индексами\n",
    "    query = f\"SELECT embeddings FROM images WHERE id IN ({','.join(map(str, indices))})\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "\n",
    "    # Преобразуем эмбеддинги в массив\n",
    "    embeddings = np.vstack(df[\"embeddings\"].apply(lambda x: np.array(x)).values)\n",
    "\n",
    "    return embeddings"
   ],
   "id": "c5b0c253ec21e0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция получения экспонатов по индексам ===\n",
    "def get_exhibits_by_indices(indices):\n",
    "    return df.iloc[indices][[\"catalog_num\", \"title\", \"author\", \"date_category\"]].to_dict(orient=\"records\")"
   ],
   "id": "afcda1362415df2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Пример использования ===\n",
    "user_query = \"картина, изображающая пейзаж с озером\"\n",
    "recommended_indices = recommend_by_kmeans_knn(user_query, kmeans_model)\n",
    "recommended_exhibits = get_exhibits_by_indices(recommended_indices)\n",
    "\n",
    "recommended_exhibits"
   ],
   "id": "7f51690177914b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# вариант 2",
   "id": "62b6668eb24458c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ],
   "id": "3c352391b7dd45ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка переменных окружения ===\n",
    "load_dotenv()\n",
    "\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USERNAME')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "\n",
    "DATABASE_URL = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Создание подключения к БД\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ],
   "id": "844dc0bc6fbfb86c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка модели KMeans ===\n",
    "kmeans_model = joblib.load(\"./data-copy/kmeans_model2.joblib\")\n",
    "\n",
    "# === Загрузка модели RuBERT ===\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ],
   "id": "1537aeb88e463d66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка модели Spacy для лемматизации ===\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "# === Список стоп-слов для TF-IDF ===\n",
    "STOP_WORDS = nlp.Defaults.stop_words"
   ],
   "id": "e3fbbe81034d7b3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция для лемматизации и очистки текста ===\n",
    "def lemmatize_text_spacy(text):\n",
    "    doc = nlp(text.lower())  # Приводим к нижнему регистру\n",
    "    lemmatized_words = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha  # Убираем числа и знаки препинания\n",
    "        and len(token.lemma_) > 1  # Исключаем отдельные буквы\n",
    "        and token.lemma_ not in STOP_WORDS  # Исключаем стоп-слова\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)"
   ],
   "id": "2259185f6f0d553d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция для получения эмбеддингов запроса ===\n",
    "def get_embeddings(query):\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()"
   ],
   "id": "accfd4718ee76ae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Загрузка эмбеддингов из PostgreSQL ===\n",
    "def load_embeddings():\n",
    "    query = \"SELECT catalog_num, title, author, date_category, embeddings FROM images\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    df['embeddings'] = df['embeddings'].apply(lambda x: np.array(x))\n",
    "    return df"
   ],
   "id": "cd7f964ace6b45e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = load_embeddings()\n",
    "embeddings = np.vstack(df[\"embeddings\"].values)"
   ],
   "id": "60eaf342e244a6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция для извлечения ключевых слов с использованием TF-IDF ===\n",
    "def extract_keywords_with_tfidf(df, top_n=15):\n",
    "    # Объединяем описание и название в единый текст и лемматизируем\n",
    "    df['combined_text'] = df['description'] + ' ' + df['title']\n",
    "    df['lemmatized_text'] = df['combined_text'].apply(lemmatize_text_spacy)\n",
    "\n",
    "    # Инициализируем TF-IDF векторизатор с русскими стоп-словами\n",
    "    vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['lemmatized_text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    df['key_words'] = ''\n",
    "\n",
    "    for index in tqdm(range(len(df)), total=len(df), desc=\"Processing rows\", ncols=100):\n",
    "        # Получаем TF-IDF веса для текущей строки\n",
    "        tfidf_vector = tfidf_matrix[index].toarray().flatten()\n",
    "        top_indices = tfidf_vector.argsort()[-top_n:][::-1]\n",
    "\n",
    "        # Извлекаем ключевые слова\n",
    "        key_words = [feature_names[i] for i in top_indices if tfidf_vector[i] > 0]\n",
    "\n",
    "        # Сохраняем ключевые слова\n",
    "        df.at[index, 'key_words'] = ','.join(key_words)\n",
    "\n",
    "    # Удаляем временные колонки\n",
    "    df.drop(columns=['combined_text', 'lemmatized_text'], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "fec8f90f743859c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция рекомендаций через KMeans + KNN ===\n",
    "def recommend_by_kmeans_knn(query, embeddings, kmeans_model, n_recommendations=5):\n",
    "    # Преобрабатываем запрос\n",
    "    preprocessed_query = lemmatize_text_spacy(query)\n",
    "\n",
    "    # Получаем эмбеддинг для обработанного запроса\n",
    "    query_embedding = get_embeddings([preprocessed_query])\n",
    "\n",
    "    # Определяем кластер запроса\n",
    "    query_cluster = kmeans_model.predict(query_embedding)[0]\n",
    "\n",
    "    # Находим индексы экспонатов из этого кластера\n",
    "    cluster_indices = np.where(kmeans_model.labels_ == query_cluster)[0]\n",
    "    cluster_embeddings = embeddings[cluster_indices]\n",
    "\n",
    "    # Используем KNN для поиска ближайших экспонатов\n",
    "    nn_model = NearestNeighbors(n_neighbors=n_recommendations, metric='cosine')\n",
    "    nn_model.fit(cluster_embeddings)\n",
    "\n",
    "    distances, indices = nn_model.kneighbors(query_embedding)\n",
    "    recommended_indices = cluster_indices[indices.flatten()]\n",
    "\n",
    "    return recommended_indices"
   ],
   "id": "8e070fbf8523577c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Функция получения экспонатов по индексам ===\n",
    "def get_exhibits_by_indices(indices):\n",
    "    return df.iloc[indices][[\"catalog_num\", \"title\", \"author\", \"date_category\"]].to_dict(orient=\"records\")"
   ],
   "id": "b5a3b247849e15b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Пример использования ===\n",
    "user_query = \"картина, изображающая пейзаж с озером\"\n",
    "recommended_indices = recommend_by_kmeans_knn(user_query, embeddings, kmeans_model)\n",
    "recommended_exhibits = get_exhibits_by_indices(recommended_indices)\n",
    "\n",
    "print(recommended_exhibits)"
   ],
   "id": "6c41f3f6514312e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Пример использования ===\n",
    "user_query = \"Хочу посмотреть изображение мужчины в историческом костюме.\"\n",
    "recommended_indices = recommend_by_kmeans_knn(user_query, embeddings, kmeans_model)\n",
    "recommended_exhibits = get_exhibits_by_indices(recommended_indices)\n",
    "\n",
    "print(recommended_exhibits)\n"
   ],
   "id": "1ffc11011ab4e166"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
